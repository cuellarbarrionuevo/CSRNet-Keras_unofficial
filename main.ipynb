{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size: 49 49 49\n",
      "Train data size: 69 69 69\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "from utils_gen import gen_paths_img_dm, gen_var_from_paths\n",
    "from utils_imgproc import norm_by_imagenet\n",
    "%matplotlib inline\n",
    "plt.ioff()\n",
    "\n",
    "from config import evitar_crash\n",
    "# evitar error del kernel muerto en portatil no en PC\n",
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "evitar_crash()\n",
    "#######################################################\n",
    "\n",
    "# Settings\n",
    "net = 'CSRNet'\n",
    "dataset = \"A\"\n",
    "# Generate paths of (train, test) x (img, dm)\n",
    "(test_img_paths, train_img_paths), (test_dm_paths, train_dm_paths) = gen_paths_img_dm(\n",
    "    path_file_root='data\\paths_train_val_test',\n",
    "    dataset=dataset\n",
    ")\n",
    "# Generate raw images(normalized by imagenet rgb) and density maps\n",
    "#test_x, test_y = gen_var_from_paths(test_img_paths[:], unit_len=None), gen_var_from_paths(test_dm_paths[:], stride=8, unit_len=None)\n",
    "test_x = gen_var_from_paths(test_img_paths[:], unit_len=16) \n",
    "test_y = gen_var_from_paths(test_dm_paths[:], stride=8, unit_len=16)\n",
    "test_x = norm_by_imagenet(test_x)  # Normalization on raw images in test set, those of training set are in image_preprocessing below.\n",
    "print('Test data size:', test_x.shape[0], test_y.shape[0], len(test_img_paths))\n",
    "train_x, train_y = gen_var_from_paths(train_img_paths[:], unit_len=16), gen_var_from_paths(train_dm_paths[:], stride=8, unit_len=16)\n",
    "print('Train data size:', train_x.shape[0], train_y.shape[0], len(train_img_paths))\n",
    "# Delete the directory for saving weights during last training.\n",
    "weights_dir = 'weights_' + dataset\n",
    "if os.path.exists(weights_dir):\n",
    "    shutil.rmtree(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oriol\\anaconda3\\envs\\aI_Keras\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "c:\\Users\\oriol\\anaconda3\\envs\\aI_Keras\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, None, None, 64)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, None, None, 128)  0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, None, None, 256)  0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, None, None, 256)   1179904   \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, None, None, 128)   295040    \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, None, None, 64)    73792     \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, None, None, 1)     65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,263,489\n",
      "Trainable params: 16,263,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "from CSRNet import CSRNet\n",
    "from config import evitar_crash, GPU\n",
    "# evitar error del kernel muerto en portatil no en PC\n",
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "evitar_crash()\n",
    "#######################################################\n",
    "\n",
    "# Create empty directory for saving weights during training\n",
    "if os.path.exists(weights_dir):\n",
    "    shutil.rmtree(weights_dir)\n",
    "os.makedirs(weights_dir)\n",
    "\n",
    "# Settings of network\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\" ### ojo sin GPU\n",
    "GPU()\n",
    "LOSS = 'MSE'\n",
    "optimizer = Adam(lr=1e-5)\n",
    "\n",
    "# Create my model\n",
    "model = CSRNet(input_shape=(None, None, 3))\n",
    "model.compile(optimizer=optimizer, loss='MSE')\n",
    "model.summary()\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "plot_model(model, 'models/{}.png'.format(net))\n",
    "with open('./models/{}.json'.format(net), 'w') as fout:\n",
    "    fout.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from time import time, ctime\n",
    "from utils_imgproc import image_preprocessing\n",
    "from utils_callback import eval_loss, callbacks_during_train\n",
    "\n",
    "\n",
    "# Settings of training\n",
    "batch_size = 1\n",
    "epoch = 250\n",
    "epoch = 2\n",
    "val_rate = 0.5\n",
    "val_rate_dec = {'A': [80, 70], 'B': [9, 8.5]}\n",
    "len_train = train_x.shape[0]\n",
    "num_iter = int((len_train-0.1) // batch_size + 1)\n",
    "best_values = {'mae': 1e5, 'rmse': 1e5, 'sfn': 1e5, 'mape': 1e5}\n",
    "losses = [[1e5, 1e5, 1e5, 1e5]]\n",
    "# Settings of display\n",
    "dis_idx = 16 if dataset == 'B' else 0\n",
    "dis_path = test_img_paths[dis_idx]\n",
    "dis_x = test_x[dis_idx]\n",
    "dis_y = test_y[dis_idx]\n",
    "dis_lim = (5, 35) if dataset == 'B' else (40, 150)\n",
    "time_st = time()\n",
    "\n",
    "# Training iterations\n",
    "for ep in range(epoch):\n",
    "    for idx_train in range(0, len_train, batch_size):\n",
    "        dis_epoch = str(ep+1)+'-'+str(idx_train+1)+'_'+str(len_train)\n",
    "        x, y = train_x[idx_train:idx_train+batch_size], train_y[idx_train:idx_train+batch_size]\n",
    "        # Preprocessings on raw images\n",
    "        x, y = image_preprocessing(\n",
    "            x, y,\n",
    "            flip_hor=True\n",
    "        )\n",
    "        model.fit(x, y, batch_size=1, verbose=0)\n",
    "        idx_val = (idx_train / batch_size + 1)\n",
    "        # Eval losses and save models\n",
    "        if idx_val % (num_iter * val_rate) == 0:\n",
    "            # To see predictions during training in directory 'tmp'\n",
    "#             callbacks_during_train(\n",
    "#                 model, dis_x=dis_x, dis_y=dis_y, dis_path=dis_path,\n",
    "#                 net=net, epoch=dis_epoch\n",
    "#             )\n",
    "            loss = eval_loss(model, test_x, test_y, quality=False)\n",
    "            if loss[0] < val_rate_dec[dataset][0]:\n",
    "                val_rate = min(val_rate, 0.25)\n",
    "            if loss[0] < val_rate_dec[dataset][1]:\n",
    "                val_rate = min(val_rate, 0.1)\n",
    "            losses.append(loss)\n",
    "            if (loss[0] < best_values['mae']) or (loss[0] == best_values['mae'] and loss[1] < best_values['rmse']):\n",
    "                model.save_weights(os.path.join(weights_dir, '{}_best.hdf5'.format(net)))\n",
    "            for idx_best in range(len(loss)):\n",
    "                if loss[idx_best] < best_values[list(best_values.keys())[idx_best]]:\n",
    "                    best_values[list(best_values.keys())[idx_best]] = loss[idx_best]\n",
    "                    to_save = True\n",
    "            if to_save:\n",
    "                path_save = os.path.join(weights_dir, ''.join([\n",
    "                    net,\n",
    "                    '_MAE', str(round(loss[0], 3)), '_RMSE', str(round(loss[1], 3)),\n",
    "                    '_SFN', str(round(loss[2], 3)), '_MAPE', str(round(loss[3], 3)),\n",
    "                    '_epoch', str(ep+1), '-', str(idx_val), '.hdf5'\n",
    "                ]))\n",
    "                model.save_weights(path_save)\n",
    "                to_save = False\n",
    "        # Progress panel\n",
    "        time_consuming = time() - time_st\n",
    "        sys.stdout.write('In epoch {}, with MAE-RMSE-SFN-MAPE={}, time consuming={}m-{}s\\r'.format(\n",
    "            dis_epoch, np.round(np.array(losses)[-1, :], 2),\n",
    "            int(time_consuming/60), int(time_consuming-int(time_consuming/60)*60)\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "# Save records\n",
    "losses = np.array(losses[1:])\n",
    "pd.DataFrame(losses).to_csv('{}/loss.csv'.format(weights_dir), index=False, header=['MAE', 'RMSE', 'SFN', 'MAPE'])\n",
    "losses_MAE, losses_RMSE, losses_SFN, losses_MAPE = losses[:, 0], losses[:, 1], losses[:, 2], losses[:, 3]\n",
    "plt.plot(losses_MAE, 'r')\n",
    "plt.plot(losses_RMSE, 'b')\n",
    "multiplier = int(round(dis_lim[0] / (np.min(losses_SFN)+0.1)))\n",
    "plt.plot(losses_SFN * multiplier, 'g')\n",
    "plt.legend(['MAE', 'RMSE', 'SFN*{}'.format(multiplier)])\n",
    "plt.ylim(dis_lim)\n",
    "plt.title('Val_losses in {} epochs'.format(epoch))\n",
    "plt.savefig('{}/{}_val_loss.png'.format(weights_dir, net))\n",
    "plt.show()\n",
    "\n",
    "# Rename weights_dir by the trainging end time, to prevent the careless deletion or overwriting\n",
    "end_time_of_train = '-'.join(ctime().split()[:-2])\n",
    "suffix_new_dir = '_{}_{}_bestMAE{}_{}'.format(dataset, LOSS, str(round(best_values['mae'], 3)), end_time_of_train)\n",
    "weights_dir_neo = 'weights'+suffix_new_dir\n",
    "shutil.move('weights_{}'.format(dataset), weights_dir_neo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analysis on results\n",
    "dis_idx = 16 if dataset == 'B' else 0\n",
    "weights_dir_neo = 'weights_A_MSE_bestMAE113.877_Sun-Jul-21'\n",
    "model = model_from_json(open('models/{}.json'.format(net), 'r').read())\n",
    "model.load_weights(os.path.join(weights_dir_neo, '{}_best.hdf5'.format(net)))\n",
    "ct_preds = []\n",
    "ct_gts = []\n",
    "for i in range(len(test_x[:])):\n",
    "    if i % 100 == 0:\n",
    "        print('{}/{}'.format(i, len(test_x)))\n",
    "    i += 0\n",
    "    test_x_display = np.squeeze(test_x[i])\n",
    "    test_y_display = np.squeeze(test_y[i])\n",
    "    path_test_display = test_img_paths[i]\n",
    "    pred = np.squeeze(model.predict(np.expand_dims(test_x_display, axis=0)))\n",
    "    ct_pred = np.sum(pred)\n",
    "    ct_gt = round(np.sum(test_y_display))\n",
    "    ct_preds.append(ct_pred)\n",
    "    ct_gts.append(ct_gt)\n",
    "plt.plot(ct_preds, 'r>')\n",
    "plt.plot(ct_gts, 'b+')\n",
    "plt.legend(['ct_preds', 'ct_gts'])\n",
    "plt.title('Pred vs GT')\n",
    "plt.show()\n",
    "error = np.array(ct_preds) - np.array(ct_gts)\n",
    "plt.plot(error)\n",
    "plt.title('Pred - GT, mean = {}, MAE={}'.format(\n",
    "    str(round(np.mean(error), 3)),\n",
    "    str(round(np.mean(np.abs(error)), 3))\n",
    "))\n",
    "plt.show()\n",
    "idx_max_error = np.argsort(np.abs(error))[::-1]\n",
    "\n",
    "# Show the 5 worst samples\n",
    "for worst_idx in idx_max_error[:50].tolist() + [dis_idx]:\n",
    "    test_x_display = np.squeeze(test_x[worst_idx])\n",
    "    test_y_display = np.squeeze(test_y[worst_idx])\n",
    "    path_test_display = test_img_paths[worst_idx]\n",
    "    pred = np.squeeze(model.predict(np.expand_dims(test_x_display, axis=0)))\n",
    "    fg, (ax_x_ori, ax_y, ax_pred) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "    ax_x_ori.imshow(cv2.cvtColor(cv2.imread(path_test_display), cv2.COLOR_BGR2RGB))\n",
    "    ax_x_ori.set_title('Original Image')\n",
    "    ax_y.imshow(test_y_display, cmap=plt.cm.jet)\n",
    "    ax_y.set_title('Ground_truth: ' + str(np.sum(test_y_display)))\n",
    "    ax_pred.imshow(pred, cmap=plt.cm.jet)\n",
    "    ax_pred.set_title('Prediction: ' + str(np.sum(pred))+\" % \"+str(round((1-(np.sum(pred))/np.sum(test_y_display))*100)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate losses and the image quality\n",
    "# model = model_from_json(open('models/{}.json'.format(net), 'r').read())\n",
    "# model.load_weights('{}/{}_best.hdf5'.format('weights_', net))\n",
    "# from utils_callback import eval_loss\n",
    "lossMAE, lossRMSE, lossSFN, lossMAPE, PSNR, SSIM = eval_loss(\n",
    "    model, test_x, test_y, quality=True\n",
    ")\n",
    "print(lossMAE, lossRMSE, lossSFN, lossMAPE, PSNR, SSIM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
